---
title: 'DATA 624 PREDICTIVE ANALYTICS - Project 2'
author: "Melissa Bowman, Frederick Jones, Shoshana Farber, Gabriel Campos"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook: default
  geometry: left=0.5cm,right=0.5cm,top=1cm,bottom=2cm
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
---

# Libraries

```{r, warning=FALSE, message=FALSE}
library(Amelia)
library(car)
library(caret)
library(corrplot)
library(Cubist)
library(DataExplorer)
library(dplyr)
library(e1071)
library(earth)
library(forcats)
library(forecast)
library(fpp3)
library(gbm)
library(ggplot2)
library(kableExtra)
library(MASS)
library(mice)
library(mlbench)
library(party)
library(randomForest)
library(RANN)
library(RColorBrewer)
library(readxl)
library(rpart)
library(rpart.plot)
library(summarytools)
library(tidyr)
library(VIM)
library(earth)
library(randomForest)
```

# Assignment Description

**Project #2 (Team) Assignment**

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of pH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.
Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.

# Data Import

We will first load in the data that is required for this analysis. 

```{r}
train_df <- readxl::read_xlsx('C:/Users/Shoshana/Documents/CUNY SPS/Data624_Collab/Data/StudentData.xlsx')
test_df <- readxl::read_xlsx('C:/Users/Shoshana/Documents/CUNY SPS/Data624_Collab/Data/StudentEvaluation.xlsx')
```

`StudentData.xlsx` is our Training data set.

`StudentEvaluation.xlsx` is our Test data set.

# Exporatory Data Analysis

First, we can preview our dataset. 

```{r}
glimpse(train_df)
```

The dataset consists of 2,571 rows and 33 columns. Most of the variables are numeric, except for the first column indicating `Brand Code`. Our response variable is `PH`. 

We can take also take a look at the summary statistics for each of the numeric variables. 

```{r}
summary(train_df)
```

### NA Proportions

We can plot the missing values for each column to see what proportion of each variable is missing.

```{r}
plot_missing(train_df, 
             missing_only = T,
             ggtheme = theme_classic(),
             theme_config = list(legend.position = c("right")),
             geom_label_args = list("size" = 3, "label.padding" = unit(0.1, "lines")))
```

We can see that majority of the variables are missing less than 1% of values. For those that are missing more than 1% of the data, majority still fall below 5%. The variable with the most missing data, and possibly cause for concern, is `MFR`. However, even this is missing only about 8.25% of the data. 

```{r}
data.frame(missing = colSums(is.na(train_df))) |>
  filter(missing == 0) |>
  rownames()
```

`Pressure Vacuum` and `Air Pressurer` are the only variables not missing any data. 

```{r, warning=FALSE}
VIM::aggr(train_df, numbers=T, sortVars=T, bars = FALSE,
          cex.axis = .6)
```


### Distributions

We will now take a look at the distributions of the numeric variables. 

```{r}
DataExplorer::plot_histogram(train_df, nrow = 4L, ncol = 4L, ggtheme = theme_classic())
```

`Carb Pressure`, `Carb Temp`, `Fill Ounces`, `PC Volume`, and `PH` seem to be relatively normally distributed. 

`Hyd Pressure 1`, `PCS`, `PSC CO2`, `PSC Fill`, `Air Pressurer`, `Oxygen Filler`,`Pressure Vacuum`, and `Temperature` all seem to have a right skew. 

`Hyd Pressure2`, `Hyd Pressure3`, and `Mnf Flow` all seem to have a left skew, although there are also a fair amount of entries with a value at 0. `Filler Speed` and `MFR` also seem to have a left skew. 

Some variables, such as `Balling`, `Balling Lvl`, `Carb Rel`, and `Density` seem to be bimodally distributed. 

### Initial Findings

* Data consists of 2571 observations with 33 columns
* `Brand Code`:
  + Type character
  + Unordered categorical values
* Predictors:
  + Primarily doubles
  + 4 can be considered integers
  + High range variables:
    i. `Mnf Flow` -100.20 to 220.40
    ii. `Hyd Pressure1` -50.00 to 50.00
    iii. `Hyd Pressure2` -50.00 to 61.40
    iv. `Hyd Pressure3` -50.00 to 49.20
    v. `Hyd Pressure4` 68.00 to 140.00
* About 8% of the values for `MFR` is missing.  
* `Brand Code` is missing about 5% 
* Filler Speed is missing about 2%
* Remaining Variables have roughly 1% or less missing.  
* `Pressure.Vacuum`, `Air.Pressurer` have no NAs
* The Distribution of the variables can be grouped as **left skewed**, **right skewed** and for symmetric we can categorized as **relatively normal**
  + Relatively Normal Distributions:
    - `Carb.Pressure`
    - `Carb.Temp`
    -`Fill.Ounces`
    - `PC.Volume`
    - `PH` 
  + Left-skew Distributions:
    - `Carb.Flow`
    - `Filler.Speed`
    - `Mnf.Flow`
    - `MFR`
    - `Bowl.Setpoint`
    - `Filler.Level`
    - `Hyd.Pressure2`
    - `Hyd.Pressure3`
    -`Usage.cont`
    - `Carb.Pressure1`
    - `Filler.Speed`
  + Right-skew Distributions:
    - `Pressure.Setpoint`
    - `Fill.Pressure`
    - `Hyd.Pressure1`
    - `Temperature`
    - `Carb.Volume`
    - `PSC`
    - `PSC.CO2`
    - `PSC.Fill`
    - `Balling`
    - `Density`
    - `Hyd.Pressure4`
    - `Air.Pressurer`
    - `Alch.Rel`
    - `Carb.Rel`
    - `Oxygen.Filler`
    - `Balling.Lvl`
    - `Pressure.Vacuum`

```{r}
unique(train_df$`Brand Code`)
```

### Brand Code Distribution

`Brand Code` has 4 categorical values outside of NA (**A,B,C,D**). Let's examine the distribution of these codes.

```{r brand_code_dist, fig.height=5, warning=F}
train_df |>
  mutate(`Brand Code` = factor(`Brand Code`, levels = names(sort(table(`Brand Code`), decreasing = TRUE)))) |>
  ggplot(aes(x = `Brand Code`, fill = `Brand Code`)) +
  geom_bar(stat = "count") +
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, color = "black") +
  labs(title = 'Brand Code Distribution', x = 'Brand Code', y = 'Frequency') +
  theme_minimal()
```

Majority of the entries in the dataset belong to `Brand Code` B. A and C have about the same number of entries. There are 120 missing values for `Brand Code`. 

### Correlation

First, we can plot a correlation matrix of our predictor variables to see which predictors are correlated with each other. 

```{r corrplot_eda, fig.height=15}
train_numeric_df <- train_df |>
  dplyr::select(where(is.numeric)) |>
  na.omit()

# Calculate correlation matrix
train_numeric_cor <- train_numeric_df |>
  dplyr::select(-PH) |>
  cor()

# Generate the correlation plot
corrplot(train_numeric_cor,
         method = "color",
         tl.col = "black",
         col = brewer.pal(n = 10,
                          name = "RdYlBu"),
         type = "lower",
         diag=FALSE,
         order = "hclust", 
         addCoef.col = "black",
         number.cex = 0.8,
         tl.cex = 0.8,
         cl.cex = 0.8,
         addCoefasPercent = TRUE,
         number.digits = 1)
```

We can see a few instances of multicollinearity in our predictor variables. `Carb Rel`, `Alch Rel`, `Density`, `Balling` and `Balling Level` are all significantly positively correlated with each other. `Hyd Pressue2` is significantly positively correlated with `Hyd Pressure 3`. Likewise, `Carb Temp` with `Carb Pressure`, `MFR` with `Fill Speed`, `Bowl Setpoint` with `Fill Level`, and `Pressure Setpoint` with `Fill Pressure`. 

There are also a number of variables that are highly negatively correlated with each other, such as `Pressure Vacuum` with `Hyd Pressure2` and `Hyd Pressure3`, `Mnf Flow` with `Filler Level` and `Bowl Setpoint`, and `Hyd Pressure4` with `Alch Rel`. 

A number of other variables also display moderate correlations with each other, as can be seen from the medium blue and medium red squares in the correlation plot. 

We will need to address these multicollinearity issues in our models. 

### PH

With `PH` being our response variable, assessing `PH`'s correlation with other variables is needed.

```{r ph_corrplot, fig.height=7}
train_numeric_df |>
  dplyr::select(-PH) |>  # Exclude 'PH' from predictors if needed
  cor(train_numeric_df$PH) |>  # Calculate correlations with 'PH'
  as.data.frame() |>
  rownames_to_column(var = "Predictor") |>
  filter(Predictor != "PH") |>  # Ensure 'PH' is not included as its own predictor
  mutate(Predictor = fct_reorder(factor(Predictor), V1)) |>  # Reorder factors by correlation for plotting
  ggplot(aes(x = Predictor, y = V1, label = round(V1, 2))) +
    geom_col(aes(fill = ifelse(V1 < 0, "negative", "positive"))) +
    geom_text(color = "black", size = 3, vjust = -0.3) +
    coord_flip() +
    labs(title = "Correlations: pH", x = "Predictors", y = "Correlation Coefficient") +
    theme_minimal() +
    theme(legend.position = "none")
```

Individually, there are no variables that are extremely correlated with `PH`. `Mnf Flow` has the largest correlation of about -0.46. The most significantly positively correlated variables with `PH` are `Bowl Setpoint` and `Filler Level`. The most significantly negatively correlated variables, other than `Mnf Flow`, are `Usage cont`, `Fill Pressure`, and `Pressure Setpoint`. 

## Data Cleanup and Pre-Processing

First, to make it easier to reference our variables, let's make each column name snakecase.

```{r}
names(train_df) <- snakecase::to_snake_case(names(train_df))
names(test_df) <- snakecase::to_snake_case(names(test_df))
```

Now, as `ph` is our target variable, we will need to remove any rows that do not have a value for this column.

```{r}
train_df <- train_df |>
  filter(!is.na(ph))
```

We will also transform our `brand_code` variable to categorized factors, replacing any NA value with "Unknown". 

```{r}
train_df <- train_df |> 
  dplyr::mutate(brand_code = ifelse(is.na(brand_code), "Unknown", brand_code),
                brand_code = factor(brand_code, levels = c('A','B','C','D','Unknown'), ordered = FALSE))

test_df <- test_df |> 
  dplyr::mutate(brand_code = ifelse(is.na(brand_code), "Unknown", brand_code),
                brand_code = factor(brand_code, levels = c('A','B','C','D','Unknown'), ordered = FALSE))
```

We will identify unhelpful columns in the dataset, such as any variables with zero variance or near zero variance. 

```{r}
nzv_df <- nearZeroVar(train_df, saveMetrics= TRUE)
nzv_df <- as.data.frame(nzv_df) %>% 
  rownames_to_column(var = "Predictor") 

nzv_filtered_df <- nzv_df %>% 
  filter(nzv == TRUE)

ggplot(nzv_filtered_df, aes(x = Predictor, y = percentUnique, fill = freqRatio > 0.95)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Near-Zero Variance Predictors", 
       x = "Predictors", 
       y = "Percentage of Unique Values") +
  theme_minimal()

print(nzv_filtered_df)
```

`hyd_pressure_1` is the only variable with near zero variance. We will not include this variable in our modeling. 

Finally, we will pre-process the data for modeling. 

The data is in the form of a tibble. For pre-processing using the `preProcess()` function from the **`caret`** package, we need the data in the form of a dataframe. We will use  `as.data.frame()` to do this.

```{r}
train_df <- as.data.frame(train_df)
test_df <- as.data.frame(test_df)
```

We will leverage `caret` package method [preProcess](https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/preProcess) to transform data using methods:
  + knnImpute - nearest neighbor to impute missing data
  + nzv = remove near-zero values identified above
  + corr = filters out highly correlated values addressing
  multicollinearity
  + center = subtracts the mean of the predictor's data (again from
  the data in x) from the predictor values 
  + scale = divides by the standard deviation.
  + BoxCox = normalizes data
* Use the `predict` function to process the list variables created with `preProcess()` to recreate the dataframe.

```{r}
#remove pH from the train data set in order to only transform the predictors
train_preprocess_df <- train_df |> 
  dplyr::select(-c(ph))

preProc_ls <- preProcess(train_preprocess_df, method = c("knnImpute", "nzv", "corr", "center", "scale", "BoxCox"))

train_preProc_df <- predict(preProc_ls, train_preprocess_df)

# rejoin with pH
train_preProc_df$ph <- train_df$ph 

#remove pH from the test data set in order to only transform the predictors
test_preprocess_df <- test_df |> 
  dplyr::select(-c(ph))

preProc_ls <- preProcess(test_preprocess_df, method = c("knnImpute", "nzv", "corr", "center", "scale", "BoxCox"))

test_preProc_df <- predict(preProc_ls, test_preprocess_df)

# rejoin with pH
test_preProc_df$ph <- test_df$ph 
```

Let's check that no missing values remain. 

```{r}
# verify no NAs remain
colSums(is.na(train_preProc_df))
```

### Data Partition

We will split the data into an 80:20 training and validation set. 

```{r}
set.seed(1234)  # for reproducibility

training_set_df <- createDataPartition(train_preProc_df$ph, p=0.8, list=FALSE)

train <- train_preProc_df[training_set_df,]
eval <- train_preProc_df[-training_set_df,]
```

We will now build several model using the data and we will evaluate each one to determine which is the best model for our data. 

# Modeling

### Linear Model

```{r}
lm <- lm(ph ~ ., data=train)

summary(lm)
```

The $R^2$ for this model is 0.394 and there are a number of insignificant variables in the model. Let's use the `step()` function to remove some of the more insignificant variables.

```{r}
lm_update <- step(lm, direction="both", trace=0)

summary(lm_update)
```

The $R^2$ value increased slightly to about 0.395.

Let's check the diagnostic plots for this model.

```{r}
par(mfrow = c(1,2))
plot(lm_update, which = c(1,2))
```

From the residuals vs fitted plot, there does not seem to be any heteroscedasticity, so constant variance is fulfilled. From the QQ-plot, the residuals seem relatively normally distributed although they diverge from the normal line toward the lower end. 

Let's evaluate how this model performs on the evaluation data. 

```{r}
lm_pred <- predict(lm_update, eval)
(lm_metrics <- postResample(lm_pred, eval$ph))
```

The evaluation set has an $RMSE$ of 0.13 and an $R^2$ of 0.39. 

### PLS Model 

```{r message=FALSE, warning=FALSE}
set.seed(2341)

# generate model
pls_model <- train(ph ~ .,
                   data=train,
                   method='pls',
                   metric='Rsquared',
                   tuneLength=10,
                   trControl=trainControl(method = "cv",  number = 10))

plot(pls_model)

pls_model
```

The optimal number of components for the PLS model was 9, with a corresponding $R^2$ of about 0.39.  

Let's take a look at the most important variables for the PLS model.

```{r}
plot(varImp(pls_model))
```

The most important variable is `mnf_flow`. 

Let's evaluate how this model performs on the evaluation data. 

```{r}
# evaluate model metrics
pls_pred <- predict(pls_model, eval)
(pls_metrics <- postResample(pls_pred, eval$ph))
```

The evaluation set for the PLS model has a slightly improved $R^2$ of 0.40. 

### KNN Model

```{r}
set.seed(613) 

knn_model <- train(ph ~ ., 
                   data = train, 
                   method = "knn",
                   tuneLength = 10)
knn_model
```

The optimal k was 13, with a corresponding $R^2$ value of 0.43. This is improved over both the linear and PLS models. 

Let's take a look at the most important variables for this model. 

```{r}
plot(varImp(knn_model))
```

For the KNN model, `oxygen_filler` is the most important variable and `mnf_filler` is the second most important variable. 

Let's evaluate how this model performs on the evaluation data. 

```{r}
knn_pred <- predict(knn_model, eval)
(knn_metrics <- postResample(knn_pred, eval$ph))
```

The KNN model performs much better than the linear and PLS models, with an $R^2$ of about 0.5 on the evaluation set. 

### MARS Model

```{r}
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

set.seed(613)

mars_model <- train(ph ~ ., 
                    data = train,
                    method = "earth",
                    tuneGrid = marsGrid,
                    trControl = trainControl(method = "cv"))

mars_model
```

The MARS model is optimal at nprune = 30 and degree = 2. The $R^2$ at this iteration is 0.47 which is not improved from the KNN model. 

Let's take a look at the most important variables for this model. 

```{r}
plot(varImp(mars_model))
```

This model has fewer important variables than the PLS and the KNN models. Like with the PLS model, `mnf_flow` is the most important variable. 

Let's evaluate how this model performs on the evaluation data. 

```{r}
mars_pred <- predict(mars_model, eval)
(mars_metrics <- postResample(mars_pred, eval$ph))
```

The evaluation set has an $R^2$ of 0.5, slightly improved over the KNN model. 

### SVM Model

```{r}
set.seed(613)

svm_model <- train(ph ~ ., 
                   data = train,
                   method = "svmRadial",
                   tuneLength = 14,
                   trControl = trainControl(method = "cv"))

svm_model
```

The optimal model has a sigma of about 0.024 and C = 4. The $R^2$ for this model is about 0.54. 

Let's take a look at the most important variables for this model. 

```{r}
plot(varImp(svm_model))
```

This model has the same important variables as the MARS model in the same order. 

Let's evaluate how this model performs on the evaluation data. 

```{r}
svm_pred <- predict(svm_model, eval)
(svm_metrics <- postResample(svm_pred, eval$ph))
```

This model is much improved from the previous models, with an $R^2$ of about 0.58 for the evaluation set. 

### Random Forest Model

```{r}
set.seed(613)

rf_model <- randomForest(ph ~ ., 
                         data = train,
                         importance = TRUE,
                         ntree = 1000)

rf_model

rf_model
```

The model explains 65% of the variability, much improved from our previous models. 

Let's take a look at the most important variables for this model. 

```{r}
varImp(rf_model) |>
  arrange(desc(Overall)) |>
  knitr::kable()
```

`brand_code` is the most important variable for this model and `mnf_flow` is the second most important. 

Let's evaluate how this model performs on the evaluation data. 

```{r}
rf_pred <- predict(rf_model, eval)
(rf_metrics <- postResample(rf_pred, eval$ph))
```

This model performs the best from all the previous models. The $R^2$ for the evaluation set is 0.69. 

Let's take a look at all the metrics together. 

```{r}
rbind(lm_metrics, pls_metrics, knn_metrics, mars_metrics, svm_metrics, rf_metrics) |>
  knitr::kable()
```

We can clearly see that the random forest model has the highest prediction accuracy when it comes to the evaluation set, with an $R^2$ of about 69%. 