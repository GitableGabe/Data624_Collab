df_summary <- dfSummary(train_df)
print(df_summary, method = "render")
# Filter out only numeric columns
numeric_df <- train_df %>% select(where(is.numeric))
# Filter out only numeric columns
numeric_df <- train_df %>% dplyr::select(where(is.numeric))
# Convert the numeric dataframe to long format
numeric_df_long <- numeric_df %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
# Calculate the number of numeric variables
num_vars <- ncol(numeric_df)
# Decide on a grid size
nrows <- ceiling(sqrt(num_vars))  # Number of rows
ncols <- ceiling(num_vars / nrows)  # Number of columns
# Create histograms
p <- ggplot(numeric_df_long, aes(x = value)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
facet_wrap(~variable, scales = "free", nrow = nrows, ncol = ncols) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(p)
DataExplorer::plot_histogram(train, nrow = 4L, ncol = 4L)
DataExplorer::plot_histogram(train, nrow = 3L, ncol = 4L)
DataExplorer::plot_histogram(train_df, nrow = 3L, ncol = 4L)
library(Amelia)
library(car)
library(caret)
library(corrplot)
library(Cubist)
library(DataExplorer)
library(dplyr)
library(e1071)
library(earth)
library(forcats)
library(forecast)
library(fpp3)
library(gbm)
library(ggplot2)
library(kableExtra)
library(MASS)
library(mice)
library(mlbench)
library(party)
library(pls)
library(randomForest)
library(RANN)
library(RColorBrewer)
library(readxl)
library(rpart)
library(rpart.plot)
library(summarytools)
library(tidyr)
library(VIM)
train_df <- readxl::read_xlsx('Data/StudentData.xlsx')
test_df <- readxl::read_xlsx('Data/StudentEvaluation.xlsx')
glimpse(train_df)
str(train_df)
summary(train_df)
glimpse(test_df)
str(test_df)
summary(test_df)
missing_train_df <- train_df %>%
summarise(across(everything(), ~mean(is.na(.)))) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "na_proportion")
# Create a bar plot using ggplot2
ggplot(missing_train_df, aes(x = variable, y = na_proportion)) +
geom_bar(stat = "identity", fill = "skyblue", color = "lightblue") +
theme_minimal() +
labs(y = "NA Proportion", x = "Variables") +
coord_flip()
VIM::aggr(train_df, numbers=T, sortVars=T, bars = FALSE,
cex.axis = .6)
DataExplorer::plot_histogram(train_df, nrow = 3L, ncol = 4L)
unique(train_df$`Brand Code`)
train_df %>%
mutate(`Brand Code` = factor(`Brand Code`, levels = names(sort(table(`Brand Code`), decreasing = TRUE)))) %>%
ggplot(aes(x = `Brand Code`, fill = `Brand Code`)) +
geom_bar(stat = "count") +
geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, color = "black") +
labs(title = 'Brand Code Distribution', x = 'Brand Code', y = 'Frequency') +
theme_minimal()
train_numeric_df <- train_df %>%
dplyr::select(where(is.numeric)) %>%
na.omit()
# Calculate correlation matrix
train_numeric_cor <- cor(train_numeric_df)
# Generate the correlation plot
corrplot(train_numeric_cor,
method = "color",
tl.col = "black",
col = brewer.pal(n = 10,
name = "RdYlBu"),
type = "lower",
order = "hclust",
addCoef.col = "black",
number.cex = 0.8,
tl.cex = 0.8,
cl.cex = 0.8,
addCoefasPercent = TRUE,
number.digits = 1)
train_numeric_df %>%
dplyr::select(-PH) %>%  # Exclude 'PH' from predictors if needed
cor(train_numeric_df$PH) %>%  # Calculate correlations with 'PH'
as.data.frame() %>%
rownames_to_column(var = "Predictor") %>%
filter(Predictor != "PH") %>%  # Ensure 'PH' is not included as its own predictor
mutate(Predictor = fct_reorder(factor(Predictor), V1)) %>%  # Reorder factors by correlation for plotting
ggplot(aes(x = Predictor, y = V1, label = round(V1, 2))) +
geom_col(fill = "lightgreen") +
geom_text(color = "black", size = 3, vjust = -0.3) +
coord_flip() +
labs(title = "Correlations: pH", x = "Predictors", y = "Correlation Coefficient") +
theme_minimal()
train_df%>%
dplyr::filter(!is.na(PH))
train_df<-train_df%>%
dplyr::filter(!is.na(PH))
train_df<- train_df %>%
dplyr::mutate(`Brand Code` = factor(`Brand Code`,
levels = c('A','B','C','D','not known'),
ordered = FALSE))
nzv_df <- nearZeroVar(train_df, saveMetrics= TRUE)
nzv_df <- as.data.frame(nzv_df) %>%
rownames_to_column(var = "Predictor")
nzv_filtered_df <- nzv_df %>%
filter(nzv == TRUE)
ggplot(nzv_filtered_df, aes(x = Predictor, y = percentUnique, fill = freqRatio > 0.95)) +
geom_col(position = "dodge") +
coord_flip() +
labs(title = "Near-Zero Variance Predictors",
x = "Predictors",
y = "Percentage of Unique Values") +
theme_minimal()
print(nzv_filtered_df)
set.seed(1234)
train_df<- as.data.frame(train_df)
#remove pH from the train data set in order to only transform the predictors
train_preprocess_df <- train_df %>%
dplyr::select(-c(PH))
preProc_ls <- preProcess(train_preprocess_df, method = c("knnImpute", "nzv", "corr", "center", "scale", "BoxCox"))
train_preProc_df <- predict(preProc_ls, train_preprocess_df)
train_preProc_df$PH <- train_df$PH
# To verify no NAs produced when recombining
train_preProc_df%>%
dplyr::filter(is.na(PH))
training_set_df <- createDataPartition(train_preProc_df$PH, p=0.8, list=FALSE)
train_proc_pls_df <- train_preProc_df[training_set_df,]
eval_proc_pls_df <- train_preProc_df[-training_set_df,]
set.seed(222)
y_train <- subset(train_proc_pls_df, select = -c(PH))
y_test <- subset(eval_proc_pls_df, select = -c(PH))
set.seed(2341)
#generate model
pls_model <- train(y_train, train_proc_df$PH,
method='pls',
metric='Rsquared',
tuneLength=10,
trControl=trainControl(method = "cv",  number = 10))
library(Amelia)
library(car)
library(caret)
library(corrplot)
library(Cubist)
library(DataExplorer)
library(dplyr)
library(e1071)
library(earth)
library(forcats)
library(forecast)
library(fpp3)
library(gbm)
library(ggplot2)
library(kableExtra)
library(MASS)
library(mice)
library(mlbench)
library(party)
library(pls)
library(randomForest)
library(RANN)
library(RColorBrewer)
library(readxl)
library(rpart)
library(rpart.plot)
library(summarytools)
library(tidyr)
library(VIM)
train_df <- readxl::read_xlsx('Data/StudentData.xlsx')
test_df <- readxl::read_xlsx('Data/StudentEvaluation.xlsx')
glimpse(train_df)
str(train_df)
summary(train_df)
glimpse(test_df)
str(test_df)
summary(test_df)
missing_train_df <- train_df %>%
summarise(across(everything(), ~mean(is.na(.)))) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "na_proportion")
# Create a bar plot using ggplot2
ggplot(missing_train_df, aes(x = variable, y = na_proportion)) +
geom_bar(stat = "identity", fill = "skyblue", color = "lightblue") +
theme_minimal() +
labs(y = "NA Proportion", x = "Variables") +
coord_flip()
VIM::aggr(train_df, numbers=T, sortVars=T, bars = FALSE,
cex.axis = .6)
DataExplorer::plot_histogram(train_df, nrow = 3L, ncol = 4L)
unique(train_df$`Brand Code`)
train_df %>%
mutate(`Brand Code` = factor(`Brand Code`, levels = names(sort(table(`Brand Code`), decreasing = TRUE)))) %>%
ggplot(aes(x = `Brand Code`, fill = `Brand Code`)) +
geom_bar(stat = "count") +
geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, color = "black") +
labs(title = 'Brand Code Distribution', x = 'Brand Code', y = 'Frequency') +
theme_minimal()
train_numeric_df <- train_df %>%
dplyr::select(where(is.numeric)) %>%
na.omit()
# Calculate correlation matrix
train_numeric_cor <- cor(train_numeric_df)
# Generate the correlation plot
corrplot(train_numeric_cor,
method = "color",
tl.col = "black",
col = brewer.pal(n = 10,
name = "RdYlBu"),
type = "lower",
order = "hclust",
addCoef.col = "black",
number.cex = 0.8,
tl.cex = 0.8,
cl.cex = 0.8,
addCoefasPercent = TRUE,
number.digits = 1)
train_numeric_df %>%
dplyr::select(-PH) %>%  # Exclude 'PH' from predictors if needed
cor(train_numeric_df$PH) %>%  # Calculate correlations with 'PH'
as.data.frame() %>%
rownames_to_column(var = "Predictor") %>%
filter(Predictor != "PH") %>%  # Ensure 'PH' is not included as its own predictor
mutate(Predictor = fct_reorder(factor(Predictor), V1)) %>%  # Reorder factors by correlation for plotting
ggplot(aes(x = Predictor, y = V1, label = round(V1, 2))) +
geom_col(fill = "lightgreen") +
geom_text(color = "black", size = 3, vjust = -0.3) +
coord_flip() +
labs(title = "Correlations: pH", x = "Predictors", y = "Correlation Coefficient") +
theme_minimal()
train_df%>%
dplyr::filter(!is.na(PH))
train_df<-train_df%>%
dplyr::filter(!is.na(PH))
train_df<- train_df %>%
dplyr::mutate(`Brand Code` = factor(`Brand Code`,
levels = c('A','B','C','D','not known'),
ordered = FALSE))
nzv_df <- nearZeroVar(train_df, saveMetrics= TRUE)
nzv_df <- as.data.frame(nzv_df) %>%
rownames_to_column(var = "Predictor")
nzv_filtered_df <- nzv_df %>%
filter(nzv == TRUE)
ggplot(nzv_filtered_df, aes(x = Predictor, y = percentUnique, fill = freqRatio > 0.95)) +
geom_col(position = "dodge") +
coord_flip() +
labs(title = "Near-Zero Variance Predictors",
x = "Predictors",
y = "Percentage of Unique Values") +
theme_minimal()
print(nzv_filtered_df)
set.seed(1234)
train_df<- as.data.frame(train_df)
#remove pH from the train data set in order to only transform the predictors
train_preprocess_df <- train_df %>%
dplyr::select(-c(PH))
preProc_ls <- preProcess(train_preprocess_df, method = c("knnImpute", "nzv", "corr", "center", "scale", "BoxCox"))
train_preProc_df <- predict(preProc_ls, train_preprocess_df)
train_preProc_df$PH <- train_df$PH
# To verify no NAs produced when recombining
train_preProc_df%>%
dplyr::filter(is.na(PH))
training_set_df <- createDataPartition(train_preProc_df$PH, p=0.8, list=FALSE)
train_proc_pls_df <- train_preProc_df[training_set_df,]
eval_proc_pls_df <- train_preProc_df[-training_set_df,]
set.seed(222)
y_train <- subset(train_proc_pls_df, select = -c(PH))
y_test <- subset(eval_proc_pls_df, select = -c(PH))
set.seed(2341)
#generate model
pls_model <- train(y_train, train_proc_pls_df$PH,
method='pls',
metric='Rsquared',
tuneLength=10,
trControl=trainControl(method = "cv",  number = 10))
#evaluate model metrics
plsPred <-predict(pls_model, newdata=y_test)
plsReSample <- postResample(pred=plsPred, obs = eval_proc_pls_df$PH)
plsReSample %>% kable() %>% kable_paper()
head(train_proc_df)
library(Amelia)
library(car)
library(caret)
library(corrplot)
library(Cubist)
library(DataExplorer)
library(dplyr)
library(e1071)
library(earth)
library(forcats)
library(forecast)
library(fpp3)
library(gbm)
library(ggplot2)
library(kableExtra)
library(MASS)
library(mice)
library(mlbench)
library(party)
library(pls)
library(randomForest)
library(RANN)
library(RColorBrewer)
library(readxl)
library(rpart)
library(rpart.plot)
library(summarytools)
library(tidyr)
library(VIM)
train_df <- readxl::read_xlsx('Data/StudentData.xlsx')
test_df <- readxl::read_xlsx('Data/StudentEvaluation.xlsx')
glimpse(train_df)
str(train_df)
summary(train_df)
glimpse(test_df)
str(test_df)
summary(test_df)
missing_train_df <- train_df %>%
summarise(across(everything(), ~mean(is.na(.)))) %>%
pivot_longer(cols = everything(), names_to = "variable", values_to = "na_proportion")
# Create a bar plot using ggplot2
ggplot(missing_train_df, aes(x = variable, y = na_proportion)) +
geom_bar(stat = "identity", fill = "skyblue", color = "lightblue") +
theme_minimal() +
labs(y = "NA Proportion", x = "Variables") +
coord_flip()
VIM::aggr(train_df, numbers=T, sortVars=T, bars = FALSE,
cex.axis = .6)
DataExplorer::plot_histogram(train_df, nrow = 3L, ncol = 4L)
unique(train_df$`Brand Code`)
train_df %>%
mutate(`Brand Code` = factor(`Brand Code`, levels = names(sort(table(`Brand Code`), decreasing = TRUE)))) %>%
ggplot(aes(x = `Brand Code`, fill = `Brand Code`)) +
geom_bar(stat = "count") +
geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5, color = "black") +
labs(title = 'Brand Code Distribution', x = 'Brand Code', y = 'Frequency') +
theme_minimal()
train_numeric_df <- train_df %>%
dplyr::select(where(is.numeric)) %>%
na.omit()
# Calculate correlation matrix
train_numeric_cor <- cor(train_numeric_df)
# Generate the correlation plot
corrplot(train_numeric_cor,
method = "color",
tl.col = "black",
col = brewer.pal(n = 10,
name = "RdYlBu"),
type = "lower",
order = "hclust",
addCoef.col = "black",
number.cex = 0.8,
tl.cex = 0.8,
cl.cex = 0.8,
addCoefasPercent = TRUE,
number.digits = 1)
train_numeric_df %>%
dplyr::select(-PH) %>%  # Exclude 'PH' from predictors if needed
cor(train_numeric_df$PH) %>%  # Calculate correlations with 'PH'
as.data.frame() %>%
rownames_to_column(var = "Predictor") %>%
filter(Predictor != "PH") %>%  # Ensure 'PH' is not included as its own predictor
mutate(Predictor = fct_reorder(factor(Predictor), V1)) %>%  # Reorder factors by correlation for plotting
ggplot(aes(x = Predictor, y = V1, label = round(V1, 2))) +
geom_col(fill = "lightgreen") +
geom_text(color = "black", size = 3, vjust = -0.3) +
coord_flip() +
labs(title = "Correlations: pH", x = "Predictors", y = "Correlation Coefficient") +
theme_minimal()
train_df%>%
dplyr::filter(!is.na(PH))
train_df<-train_df%>%
dplyr::filter(!is.na(PH))
train_df<- train_df %>%
dplyr::mutate(`Brand Code` = factor(`Brand Code`,
levels = c('A','B','C','D','not known'),
ordered = FALSE))
nzv_df <- nearZeroVar(train_df, saveMetrics= TRUE)
nzv_df <- as.data.frame(nzv_df) %>%
rownames_to_column(var = "Predictor")
nzv_filtered_df <- nzv_df %>%
filter(nzv == TRUE)
ggplot(nzv_filtered_df, aes(x = Predictor, y = percentUnique, fill = freqRatio > 0.95)) +
geom_col(position = "dodge") +
coord_flip() +
labs(title = "Near-Zero Variance Predictors",
x = "Predictors",
y = "Percentage of Unique Values") +
theme_minimal()
print(nzv_filtered_df)
set.seed(1234)
train_df<- as.data.frame(train_df)
#remove pH from the train data set in order to only transform the predictors
train_preprocess_df <- train_df %>%
dplyr::select(-c(PH))
preProc_ls <- preProcess(train_preprocess_df, method = c("knnImpute", "nzv", "corr", "center", "scale", "BoxCox"))
train_preProc_df <- predict(preProc_ls, train_preprocess_df)
train_preProc_df$PH <- train_df$PH
# To verify no NAs produced when recombining
train_preProc_df%>%
dplyr::filter(is.na(PH))
training_set_df <- createDataPartition(train_preProc_df$PH, p=0.8, list=FALSE)
train_proc_df <- train_preProc_df[training_set_df,]
eval_proc_df <- train_preProc_df[-training_set_df,]
train_proc_pls_df<-train_proc_df
eval_proc_pls_df<-eval_proc_df
set.seed(222)
y_train <- subset(train_proc_pls_df, select = -c(PH))
y_test <- subset(eval_proc_pls_df, select = -c(PH))
set.seed(2341)
#generate model
pls_model <- train(y_train, train_proc_pls_df$PH,
method='pls',
metric='Rsquared',
tuneLength=10,
trControl=trainControl(method = "cv",  number = 10))
#evaluate model metrics
plsPred <-predict(pls_model, newdata=y_test)
plsReSample <- postResample(pred=plsPred, obs = eval_proc_pls_df$PH)
plsReSample %>% kable() %>% kable_paper()
head(train_proc_df)
head(eval_proc_df)
eval_proc_df <- na.omit(eval_proc_df)
train_proc_df <- na.omit(train_proc_df)
# Install and load the necessary packages
library(pls)
# Assuming train_proc_df contains your training data
# Fit PLSR model
plsr_model <- plsr(PH ~ ., data = train_proc_df, ncomp = 10)  # Set a reasonable maximum number of components
# Extract the proportion of variance explained by each component
variance_explained <- summary(plsr_model)$val$prop
# Create a scree plot
plot(1:length(variance_explained), variance_explained, type = "b",
xlab = "Number of Components", ylab = "Proportion of Variance Explained",
main = "Scree Plot for PLSR")
# Add a horizontal line at 0.05 for reference (adjust as needed)
abline(h = 0.05, col = "red", lty = 2)
# Add text indicating the percentage of variance explained by each component
text(1:length(variance_explained), variance_explained,
labels = paste0(round(variance_explained * 100, 2), "%"),
pos = 3, cex = 0.8)
# Add a legend
legend("topright", legend = "Threshold (e.g., 0.05)", lty = 2, col = "red", bty = "n")
# Create a scree plot for the first few components
plot(1:length(variance_explained), variance_explained, type = "b",
xlab = "Number of Components", ylab = "Proportion of Variance Explained",
main = "Scree Plot for PLSR")
# Zoom in on the first few components (adjust xlim as needed)
xlim <- c(1, min(10, length(variance_explained)))  # Adjust the maximum number of components if needed
plot(1:length(variance_explained), variance_explained, type = "b",
xlab = "Number of Components", ylab = "Proportion of Variance Explained",
main = "Scree Plot for PLSR", xlim = xlim)
# Add a horizontal line at 0.05 for reference (adjust as needed)
abline(h = 0.05, col = "red", lty = 2)
# Add text indicating the percentage of variance explained by each component
text(1:length(variance_explained), variance_explained,
labels = paste0(round(variance_explained * 100, 2), "%"),
pos = 3, cex = 0.8)
# Add a legend
legend("topright", legend = "Threshold (e.g., 0.05)", lty = 2, col = "red", bty = "n")
# Fit PLSR model
plsr_model <- plsr(PH ~ ., data = train_proc_df, ncomp = 5)  # Specify the number of components (e.g., 5)
summary(plsr_model)
# Predict PH values for evaluation/test set
predictions <- predict(plsr_model, newdata = eval_proc_df)
plot(plsr_model)
sum(is.na(predictions))
# Calculate Mean Squared Error (MSE)
on <- predictions - eval_proc_df$PH
on <- on^2
mse <- mean(on)
mse# Calculate R-squared (R²)
actual <- eval_proc_df$PH
ss_total <- sum((actual - mean(actual))^2)
ss_residual <- sum((actual - predictions)^2)
r_squared <- 1 - (ss_residual / ss_total)
# Print MSE and R²
cat("Mean Squared Error (MSE):", mse, "\n")
cat("R-squared (R²):", r_squared, "\n")
# Fit regression tree model to training data
tree_model <- rpart(PH ~ ., data = train_proc_df)
# Make predictions on evaluation/test data
tree_predictions <- predict(tree_model, newdata = eval_proc_df)
# Calculate Mean Squared Error (MSE)
tree_mse <- mean((tree_predictions - eval_proc_df$PH)^2)
# Calculate R-squared (R²)
tree_actual <- eval_proc_df$PH
tree_ss_total <- sum((tree_actual - mean(tree_actual))^2)
tree_ss_residual <- sum((tree_actual - tree_predictions)^2)
tree_r_squared <- 1 - (tree_ss_residual / tree_ss_total)
# Print MSE and R²
cat("Regression Tree Model:\n")
cat("Mean Squared Error (MSE):", tree_mse, "\n")
cat("R-squared (R²):", tree_r_squared, "\n")
summary(tree_model)
